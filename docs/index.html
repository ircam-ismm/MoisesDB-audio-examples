<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Audio Demo: MoisesDB</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      padding: 20px;
    }
    .category-title {
      font-size: 1.5em;
      margin: 20px 0 10px;
      color: #333;
      text-align: center;
    }
    .memory-guide-row, .grid {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 10px;
      margin: 10px 0;
    }
    .audio-container {
      background: white;
      padding: 10px;
      border-radius: 8px;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.08);
      text-align: center;
      width: 30%;
      min-width: 200px;
    }
    .comment-box {
      display: block;
      width: 90%;
      margin: 20px auto;
    }
    .audio-container.large-audio {
      min-width: 300px;
      min-height: 120px;
      padding: 10px;
      box-sizing: border-box;
    }

    .comment-box {
      width: 100%;
      min-height: 60px;
      margin-top: 10px;
      padding: 8px;
      resize: vertical;
      font-size: 0.9rem;
    }

    textarea {
      width: 100%;
      padding: 10px;
      border-radius: 6px;
      border: 1px solid #ccc;
      resize: vertical;
    }
    hr.separator {
      border: none;
      border-top: 2px dashed #ccc;
      margin: 40px auto;
      width: 80%;
    }
    .summary-section {
      background: #ffffff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #007BFF;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 1200px;
      line-height: 1.5;
    }
    .summary-section h3 {
      text-align: center;
      margin-bottom: 10px;
      color: #333;
    }
    .summary-section p {
      margin-bottom: 10px;
      text-align: justify;
    }
    .figures-section {
      background: #fff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #28a745;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 900px;
    }
    .figures-section h3 {
      text-align: center;
      margin-bottom: 15px;
      color: #333;
    }
    .figure {
      margin-bottom: 25px;
      text-align: center;
    }
    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
    }
    .caption {
      font-size: 0.9em;
      color: #555;
      margin-top: 5px;
      font-style: italic;
    }
    .matrix-grid {
      display: grid;
      grid-template-columns: auto repeat(3, 1fr);
      grid-template-rows: auto repeat(3, auto);
      gap: 10px;
      align-items: center;
      justify-items: center;
      margin: 20px 0;
    }

    .matrix-header {
      font-weight: bold;
      text-align: center;
    }

  </style>
</head>
<body>
  <h2 style="text-align: center;">Demo page of paper : â€œ Bujard Balthazar, Nika JÃ©rÃ´me, Obin Nicolas, Bevilacqua FrÃ©dÃ©ric,
     Learning Relationships between Separate Audio Tracks for Creative Applications, Proceedings of the 6th Conference on 
     AI Music Creativity (AIMC 2025), 2025"</h2>
  <!-- ðŸ”¹ Summary Section -->
<div class="summary-section">
  <p>
    This web page presents the main results from the 2025 AIMC paper "Learning Relationships between Separate Audio Tracks 
    for Creative Applications" (Bujard et al., 2025), along with some audio examples. The related code can be found 
    <a href="https://github.com/ircam-ismm/learning-from-paired-tracks">here</a>.
  </p>
  <p>
    This work explored the possibility to include learned relationships in the decision process of Musical Agents.
    The training of the different model configurations converged for both datasets, and the symbolic performances
    on the re-generation task prove that some relationships have been learned. The evaluation on each dataset leads
    to True Positive Percentage (TPP) between 10% and 20% of the learned relationships. Such percentage values,
    which might appear as low, was expected, since the TPP values do not take into account any specificity of the
    different tracks, but rather to general "musical relationships" that are emerging globally in each database.
    Thus, the important finding is that we can confirm such general relationship are learned, as shown by the fact
    that the values are significantly larger than a random generation (<em>p</em>-value &lt; 0.0001).
  </p>
  <p>
    Therefore, what has been learned must be high-level, common relationships among all paired tracks, i.e.
    corpus-level relationships. Moreover, such learning appears in both stylistically very different datasets,
    MoisesDB and MICA, related to different music genre (pop/rock and free improvisation). This observation allows
    us to, at least partially, validate the interest of our framework.
  </p>
  <p>
    The segmentation size appears to have little impact for the TPP. Instead, increased vocabulary size generally 
    correlates with reduced performance likely due to the difficulty of modeling fine-grained relationships. 
    Large variance observed across configurations indicates a 
    pronounced variability in the modelâ€™s performances.
  </p>
  <p>
    While LCP results outperform random baseline, the prefix lengths remain low, indicating difficulty in anticipating 
    future tokens. MoisesDB â€” composed of Western pop music, i.e., pulsed music â€” appears to require a 
    segmentation window that aligns with the beat (pulse) of the track. While LCP shows no consistent trend across 
    segment duration or alphabet size for MoisesDB, we find that A256/0.25s approaches the maximum LCP value.
  </p>
  <p>
    The audio examples below are composed of : 'original', 'memory' and 'source' audios corresponding to the original mix, 
    the memory used by Dicy2 and the input of the model. Then, the output grid of all 9 configurations ([window size]s A[alphabet size]).
    Each example is completed with a comment summarizing the results of highlighted examples. The ID following the "Example :" corresponds 
    to the name of the track folder in the MoisesDB dataset.
  </p>
</div>

<!-- ðŸ”¹ Figures Section -->
<div class="figures-section">
  <h3>Model summary</h3>
  <div class="figure">
    <img src="figures/schema page demo/schema page demo.001.jpeg" alt="Figure 1" />
    <p class="caption">Figure 1 â€“ Model summary : audio input source is encoded by the Perception module, the Decision learns symbolic relationships from a 
      datasate of paired tracks and predicts a symbolic specification given the encoded input source, the Action module uses the symbolic specification and a 
      'memory' audio file to generate the musical response with corpus-based concatenative synthesis.
      
    </p>
  </div>
</div>

<div id="audioContent"></div>

<script>
const root = 'moises';

const allConfigs = [
  "0.25s_A16", "0.35s_A16", "0.5s_A16",
  "0.25s_A64", "0.35s_A64", "0.5s_A64",
  "0.25s_A256", "0.35s_A256", "0.5s_A256"
];

const examplesNice = [
  {
    name: "0e0d",
    basePath: "0e0d",
    topFiles: ['guide.mp3','memory.mp3'],
    path : `moises/0e0d/0.5s_A16.mp3`,
  },
  {
    name: "747d",
    basePath: "747d",
    topFiles: ['guide.mp3','memory.mp3'],
    path : `moises/747d/0.5s_A16.mp3`,
  },
  {
    name: "f010",
    basePath: "f010",
    topFiles: ['guide.mp3','memory.mp3'],
    path : `moises/f010/0.5s_A64.mp3`,
  },
];

const exampleSets = [
  {
    name: "6c70",
    basePath: "6c70",
    topFiles: ['guide.mp3','memory.mp3'],
    examples: [
      { path:`moises/${name}/0.5s_A16.mp3`, accuracy: 18, max_len: 4, entropy:2.1},
      { path:`moises/${name}/0.5s_A256.mp3`, accuracy:10, max_len:4, entropy:1.8},
      { path:`moises/${name}/0.5s_A64.mp3`, accuracy:17, max_len:12, entropy:2}
    ],
    comment: "HArmonic relationship and mode change example. The input source is the piano and the guitar is the memory. \
    First of all, the 0.5s A16 example is not synchronized with the input source. This indicates that even a uniform segmentation window \
    does not guarantee synchronization between the response and the source. On the 500ms column, we observe that as the size of the alphabet \
    increases, the response becomes more silent. This is due to two scenarios : a silent segment is volountarly selected by the model or no \
    match in the memory was found for that symbolic specification. In this example A64 = no match and A256 = volountary silence."
  },
  {
    name: "0358",
    basePath: "0358",
    topFiles: ['guide.mp3','memory.mp3'],
    examples: [
      { path:`moises/${name}/0.5s_A16.mp3`, accuracy: 33, max_len: 9, entropy:1.9 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment : "Intensity follow-up example. The piano is the input source and the guitar serves as memory. There is no general comment on the\
    comparison between configurations. But, it can be noted that the 500ms window and alpabet size = 16 configuration reproduces the intensity follow-up interaction. \
    response starts calmly and when the piano gains in intensity the response follows this evolution. The response aloso presents well suited harmonic relaionship with \
    the input source. This shows that this configuration, not only learned harmonic relatioonships but also how to follow intensity variations. \
    This observation prompts us to investigate what is encoded in the tokens coming from the Perception module."
  },
  {
    name: "04f2",
    basePath: "04f2",
    topFiles: ['guide.mp3','memory.mp3'],
    examples: [
      { path:`moises/${name}/0.5s_A16.mp3`, accuracy: 9, max_len: 7, entropy:0.88},
      { path:`moises/${name}/0.5s_A256.mp3`, accuracy:21, max_len:6, entropy:1.7},
      { path:`moises/${name}/0.5s_A64.mp3`, accuracy:5, max_len:6, entropy:1.6}
    ],
    comment:"Lead - Accompaniment relationship example. The piano is the input source and corresponds to the Lead instrument, the bass is the accompaniment \
    instrument serving as memory. We note that the 500ms and 16 alphabet size configuration does not work; the chosen segemnt is not in the correct tonality. But, \
    when incresing the alphabet size the response is more coherent (harmonicity between source and response). This hints that a small vocabulary \
    does not allow the model to select segments in the correct tonality, maybe because the class distribution is too coarse. While larger alphabets enable the \
    selection of segemnts in the correct tonality."
  },
];

const container = document.getElementById("audioContent");

// Section title
const title = document.createElement("div");
title.className = "category-title";
title.textContent = "Generated examples";
container.appendChild(title);

// // Section text
// const text = document.createElement("div");
// text.className = "summary-section";
// text.textContent = "Below are presented 3 'good' examples";
// container.appendChild(text);

examplesNice.forEach(example => {
  // Example title
  const exampleTitle = document.createElement("h3");
  exampleTitle.className = "category-title";
  exampleTitle.textContent = `Example: ${example.name}`;
  container.appendChild(exampleTitle);

  // Row containing Memory, Guide, Output
  const exampleRow = document.createElement("div");
  exampleRow.className = "memory-guide-row"; // flex container for all three

  // Memory & Guide
  example.topFiles.forEach(file => {
    const fileContainer = document.createElement("div");
    fileContainer.className = "audio-container";

    const label = document.createElement("p");
    label.textContent = file.replace(".mp3", "").replace("_", " ");

    const audio = document.createElement("audio");
    audio.controls = true;
    audio.preload = "none";
    audio.src = `moises/${example.basePath}/${file}`;

    fileContainer.appendChild(label);
    fileContainer.appendChild(audio);
    exampleRow.appendChild(fileContainer);
  });

  // Output
  const outputContainer = document.createElement("div");
  outputContainer.className = "audio-container";

  const outputLabel = document.createElement("p");
  outputLabel.textContent = "Output";

  const outputAudio = document.createElement("audio");
  outputAudio.controls = true;
  outputAudio.preload = "none";
  outputAudio.src = example.path;

  outputContainer.appendChild(outputLabel);
  outputContainer.appendChild(outputAudio);
  exampleRow.appendChild(outputContainer);

  // Append row
  container.appendChild(exampleRow);

});

// Separator
const separator = document.createElement("hr");
separator.className = "separator";
container.appendChild(separator);

// New section title
const newSectionTitle = document.createElement("div");
newSectionTitle.className = "category-title";
newSectionTitle.textContent = "Generated examples for further discussion";
container.appendChild(newSectionTitle);

exampleSets.forEach(set => {
  const section = document.createElement('div');
  section.className = 'category-section';

  // Example title
  const title = document.createElement('div');
  title.className = 'category-title';
  title.textContent = `Example: ${set.name}`;
  section.appendChild(title);

  // Comment box
  if (set.comment) {
    const commentBox = document.createElement('div');
    commentBox.className = 'summary-section';
    commentBox.textContent = set.comment;
    section.appendChild(commentBox);
  }

  // Top files (Memory + Guide row)
  const topRow = document.createElement('div');
  topRow.className = 'memory-guide-row';
  set.topFiles.forEach(file => {
    const fileContainer = document.createElement('div');
    fileContainer.className = 'audio-container largfe-audio';

    const label = document.createElement('p');
    label.textContent = file.replace('.mp3', '').replace('_',' ');

    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = `${root}/${set.basePath}/${file}`;
    audio.preload = "none";

    fileContainer.appendChild(label);
    fileContainer.appendChild(audio);
    topRow.appendChild(fileContainer);
  });
  section.appendChild(topRow);

  // Grid with headers
  const matrixContainer = document.createElement('div');
  matrixContainer.className = 'matrix-grid';

  // First row (empty corner + column headers)
  matrixContainer.appendChild(document.createElement('div')); // empty top-left
  ['250ms', '350ms', '500ms'].forEach(header => {
    const headerDiv = document.createElement('div');
    headerDiv.className = 'matrix-header';
    headerDiv.textContent = header;
    matrixContainer.appendChild(headerDiv);
  });

  // Rows
  ['16', '64', '256'].forEach((rowLabel, rowIndex) => {
    // Row header
    const rowHeaderDiv = document.createElement('div');
    rowHeaderDiv.className = 'matrix-header';
    rowHeaderDiv.textContent = rowLabel;
    matrixContainer.appendChild(rowHeaderDiv);

    // Audio cells for this row
    ['250ms', '350ms', '500ms'].forEach((colLabel, colIndex) => {
      const cfgIndex = rowIndex * 3 + colIndex;
      const cfg = allConfigs[cfgIndex];

      const audioBox = document.createElement('div');
      audioBox.className = 'audio-container large-audio';

      const label = document.createElement('p');
      label.textContent = cfg.replace("_"," ");

      const audio = document.createElement('audio');
      audio.controls = true;
      audio.src = `${root}/${set.basePath}/${cfg}.mp3`;
      audio.preload = "none";

      // Metrics if present
      const metrics = document.createElement('div');
      metrics.className = 'metrics';
      const highlightExample = set.examples.find(e => e.path.includes(cfg));
      if (highlightExample) {
        metrics.innerHTML = `
          <small><b>Accuracy:</b> ${(highlightExample.accuracy).toFixed(1)}%</small><br>
          <small><b>Max Len:</b> ${highlightExample.max_len}</small><br>
          <small><b>Entropy:</b> ${highlightExample.entropy.toFixed(2)} [Bits]</small>
        `;
        audioBox.style.fontWeight = 'bold';
        audioBox.style.color = '#000';
      } else {
        audioBox.style.opacity = '0.7';
      }

      audioBox.appendChild(label);
      audioBox.appendChild(audio);
      audioBox.appendChild(metrics);
      matrixContainer.appendChild(audioBox);
    });
  });

  section.appendChild(matrixContainer);
  section.appendChild(document.createElement('hr'));
  container.appendChild(section);
});



</script>
</body>
</html>
