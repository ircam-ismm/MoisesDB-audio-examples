<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Audio Demo: MoisesDB</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      padding: 20px;
    }
    .category-title {
      font-size: 1.5em;
      margin: 20px 0 10px;
      color: #333;
      text-align: center;
    }
    .memory-guide-row, .grid {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 10px;
      margin: 10px 0;
    }
    .audio-container {
      background: white;
      padding: 10px;
      border-radius: 8px;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.08);
      text-align: center;
      width: 30%;
      min-width: 200px;
    }
    .comment-box {
      display: block;
      width: 90%;
      margin: 20px auto;
    }
    textarea {
      width: 100%;
      padding: 10px;
      border-radius: 6px;
      border: 1px solid #ccc;
      resize: vertical;
    }
    hr.separator {
      border: none;
      border-top: 2px dashed #ccc;
      margin: 40px auto;
      width: 80%;
    }
    .summary-section {
      background: #ffffff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #007BFF;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 900px;
      line-height: 1.5;
    }
    .summary-section h3 {
      text-align: center;
      margin-bottom: 10px;
      color: #333;
    }
    .summary-section p {
      margin-bottom: 10px;
      text-align: justify;
    }
    .figures-section {
      background: #fff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #28a745;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 900px;
    }
    .figures-section h3 {
      text-align: center;
      margin-bottom: 15px;
      color: #333;
    }
    .figure {
      margin-bottom: 25px;
      text-align: center;
    }
    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
    }
    .caption {
      font-size: 0.9em;
      color: #555;
      margin-top: 5px;
      font-style: italic;
    }


  </style>
</head>
<body>
  <h2 style="text-align: center;">Audio Demo for MoisesDB</h2>
  <!-- ðŸ”¹ Summary Section -->
<div class="summary-section">
  <p>
    This web page presents the main results from the 2025 AIMC paper "Learning Relationships between Separate Audio Tracks 
    for Creative Applications" (Bujard et al., 2025), along with some audio examples. The related code can be found 
    <a href="https://github.com/ircam-ismm/learning-from-paired-tracks">here</a>.
  </p>
  <p>
    This work explored the possibility to include learned relationships in the decision process of Musical Agents.
    The training of the different model configurations converged for both datasets, and the symbolic performances
    on the re-generation task prove that some relationships have been learned. The evaluation on each dataset leads
    to True Positive Percentage (TPP) between 10% and 20% of the learned relationships. Such percentage values,
    which might appear as low, was expected, since the TPP values do not take into account any specificity of the
    different tracks, but rather to general "musical relationships" that are emerging globally in each database.
    Thus, the important finding is that we can confirm such general relationship are learned, as shown by the fact
    that the values are significantly larger than a random generation (<em>p</em>-value &lt; 0.0001).
  </p>
  <p>
    Therefore, what has been learned must be high-level, common relationships among all paired tracks, i.e.
    corpus-level relationships. Moreover, such learning appears in both stylistically very different datasets,
    MoisesDB and MICA, related to different music genre (pop/rock and free improvisation). This observation allows
    us to, at least partially, validate the interest of our framework.
  </p>
  <p>
    The segmentation size appears to have little impact for the TPP. Instead, increased vocabulary size generally 
    correlates with reduced performance likely due to the difficulty of modeling fine-grained relationships. 
    Large variance observed across configurations (as shown by the wide boxplots on TPP values) indicates a 
    pronounced variability in the modelâ€™s performances.
  </p>
  <p>
    While LCP results outperform random baseline, the prefix lengths remain low, indicating difficulty in anticipating 
    future tokens. MoisesDB â€” composed of Western pop music, i.e., pulsed music â€” appears to require a 
    segmentation window that aligns with the beat (pulse) of the track. While LCP shows no consistent trend across 
    segment duration or alphabet size for MoisesDB, we find that A256/0.25s approaches the maximum LCP value.
  </p>
  <p>
    Below are the figures for the performances of the system for MoisesDB dataset under the constrained generation setup.
  </p>
</div>

<!-- ðŸ”¹ Figures Section -->
<div class="figures-section">
  <h3>Main Figures</h3>

  <div class="figure">
    <img src="figures/moises_tpp_fe.png" alt="Figure 1" />
    <p class="caption">Figure 1 â€“ True Positive Percentage on MoisesDB dataset under the constrained generation setup.</p>
  </div>

  <div class="figure">
    <img src="figures/moises_lcp_fe_2.png" alt="Figure 2" />
    <p class="caption">Figure 2 â€“ Longest Common Prefix on MoisesDB dataset under the constrained generation setup.</p>
  </div>
</div>


  <div id="audioContent"></div>

  <script>
  const root = 'moises';

  const exampleSets = [
    {
      name: "example 1",
      basePath: "0611",
      topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
      examples: [
        'moises/0611/0.35s_A16.mp3',
        'moises/0611/0.25s_A64.mp3',
        'moises/0611/0.5s_A256.mp3',
      ],
      comment: "Despite a memory with different notes of varying duration, all responses are monotone - \
      no matter how large the alphabet. Interestingly, both A16 and A64 choose the same note in response,\
       just A16 adds a little variation. A256 repeats 2 notes over the whole response, different from \
       those chosen by the other alphabets, but these 2 notes are part of the same segment. \
       In this example, we can see that alphabet size has no significant impact on the response generated."

    },
    {
      name: "example 2",
      basePath: "524a",
      topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
      examples: [
        'moises/524a/0.5s_A16.mp3',
        'moises/524a/0.5s_A64.mp3',
        'moises/524a/0.5s_A256.mp3',
      ],
      comment: "Generally speaking, all 3 alphabets tend to speak up at first, then fall silent. More specifically,\
       it looks as if these configs - in this example - mainly speak up during silences.\
      A16 presents long continuous sequences (good accuracy) and the larger the alphabet \
      the less continuous the sequences, confirming the increasing difficulty of modeling symbolic sequences with larger alphabets.\
      After checking in info.txt the silent segments have been well chosen by the model and this is not -1 (no match found)."

    },
    {
      name: "example 3",
      basePath: "0e0d",
      topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
      examples: [
        'moises/0e0d/0.25s_A16.mp3',
        'moises/0e0d/0.5s_A64.mp3',
        'moises/0e0d/0.35s_A256.mp3'
      ],
      comment: "comment for example 3..."
    }
  ];


  const content = document.getElementById('audioContent');

exampleSets.forEach(set => {
  const section = document.createElement('div');
  section.className = 'category-section';

  const title = document.createElement('div');
  title.className = 'category-title';
  title.textContent = `${set.name}`;
  section.appendChild(title);

  // ðŸ”¹ Top files (original, memory, guide)
  const topRow = document.createElement('div');
  topRow.className = 'memory-guide-row';

  set.topFiles.forEach(file => {
    const container = document.createElement('div');
    container.className = 'audio-container';

    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = `moises/${set.basePath}/${file}`;
    audio.preload = "none";

    const label = document.createElement('p');
    label.textContent = file.replace('.mp3', '');

    container.appendChild(label);
    container.appendChild(audio);
    topRow.appendChild(container);
  });

  section.appendChild(topRow);

  // ðŸ”¹ Grid of 3 examples
  const grid = document.createElement('div');
  grid.className = 'grid';

  set.examples.forEach(filePath => {
    const container = document.createElement('div');
    container.className = 'audio-container';

    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = filePath;
    audio.preload = "none";

    const label = document.createElement('p');
    const fileName = filePath.split('/').pop().replace('.mp3', '').replace("_"," ");
    label.textContent = fileName;

    container.appendChild(label);
    container.appendChild(audio);
    grid.appendChild(container);
  });

  section.appendChild(grid);

  // ðŸ”¹ Comment box
  if (set.comment) {
    const commentBox = document.createElement('div');
    commentBox.style.marginTop = '15px';
    commentBox.style.padding = '10px';
    commentBox.style.backgroundColor = '#fefefe';
    commentBox.style.borderLeft = '4px solid #007BFF';
    commentBox.style.fontStyle = 'italic';
    commentBox.textContent = set.comment;
    section.appendChild(commentBox);
  }

  // ðŸ”¹ Separator
  section.appendChild(document.createElement('hr')).className = 'separator';

  content.appendChild(section);
});

</script>

</body>
</html>
