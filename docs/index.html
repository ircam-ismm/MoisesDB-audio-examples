<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Audio Demo: MoisesDB</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      padding: 20px;
    }
    .category-title {
      font-size: 1.5em;
      margin: 20px 0 10px;
      color: #333;
      text-align: center;
    }
    .memory-guide-row, .grid {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 10px;
      margin: 10px 0;
    }
    .audio-container {
      background: white;
      padding: 10px;
      border-radius: 8px;
      box-shadow: 0 0 8px rgba(0, 0, 0, 0.08);
      text-align: center;
      width: 30%;
      min-width: 200px;
    }
    .comment-box {
      display: block;
      width: 90%;
      margin: 20px auto;
    }
    textarea {
      width: 100%;
      padding: 10px;
      border-radius: 6px;
      border: 1px solid #ccc;
      resize: vertical;
    }
    hr.separator {
      border: none;
      border-top: 2px dashed #ccc;
      margin: 40px auto;
      width: 80%;
    }
    .summary-section {
      background: #ffffff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #007BFF;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 900px;
      line-height: 1.5;
    }
    .summary-section h3 {
      text-align: center;
      margin-bottom: 10px;
      color: #333;
    }
    .summary-section p {
      margin-bottom: 10px;
      text-align: justify;
    }
    .figures-section {
      background: #fff;
      padding: 15px 20px;
      margin: 20px auto;
      border-left: 4px solid #28a745;
      box-shadow: 0 0 8px rgba(0,0,0,0.08);
      border-radius: 8px;
      max-width: 900px;
    }
    .figures-section h3 {
      text-align: center;
      margin-bottom: 15px;
      color: #333;
    }
    .figure {
      margin-bottom: 25px;
      text-align: center;
    }
    .figure img {
      max-width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
    }
    .caption {
      font-size: 0.9em;
      color: #555;
      margin-top: 5px;
      font-style: italic;
    }
  </style>
</head>
<body>
  <h2 style="text-align: center;">Demo page of paper : â€œ Bujard Balthazar, Nika JÃ©rÃ´me, Obin Nicolas, Bevilacqua FrÃ©dÃ©ric,
     Learning Relationships between Separate Audio Tracks for Creative Applications, Proceedings of the 6th Conference on 
     AI Music Creativity (AIMC 2025), 2025"</h2>
  <!-- ðŸ”¹ Summary Section -->
<div class="summary-section">
  <p>
    This web page presents the main results from the 2025 AIMC paper "Learning Relationships between Separate Audio Tracks 
    for Creative Applications" (Bujard et al., 2025), along with some audio examples. The related code can be found 
    <a href="https://github.com/ircam-ismm/learning-from-paired-tracks">here</a>.
  </p>
  <p>
    This work explored the possibility to include learned relationships in the decision process of Musical Agents.
    The training of the different model configurations converged for both datasets, and the symbolic performances
    on the re-generation task prove that some relationships have been learned. The evaluation on each dataset leads
    to True Positive Percentage (TPP) between 10% and 20% of the learned relationships. Such percentage values,
    which might appear as low, was expected, since the TPP values do not take into account any specificity of the
    different tracks, but rather to general "musical relationships" that are emerging globally in each database.
    Thus, the important finding is that we can confirm such general relationship are learned, as shown by the fact
    that the values are significantly larger than a random generation (<em>p</em>-value &lt; 0.0001).
  </p>
  <p>
    Therefore, what has been learned must be high-level, common relationships among all paired tracks, i.e.
    corpus-level relationships. Moreover, such learning appears in both stylistically very different datasets,
    MoisesDB and MICA, related to different music genre (pop/rock and free improvisation). This observation allows
    us to, at least partially, validate the interest of our framework.
  </p>
  <p>
    The segmentation size appears to have little impact for the TPP. Instead, increased vocabulary size generally 
    correlates with reduced performance likely due to the difficulty of modeling fine-grained relationships. 
    Large variance observed across configurations indicates a 
    pronounced variability in the modelâ€™s performances.
  </p>
  <p>
    While LCP results outperform random baseline, the prefix lengths remain low, indicating difficulty in anticipating 
    future tokens. MoisesDB â€” composed of Western pop music, i.e., pulsed music â€” appears to require a 
    segmentation window that aligns with the beat (pulse) of the track. While LCP shows no consistent trend across 
    segment duration or alphabet size for MoisesDB, we find that A256/0.25s approaches the maximum LCP value.
  </p>
  <p>
    The audio examples below are composed of : 'original', 'memory' and 'source' audios corresponding to the original mix, 
    the memory used by Dicy2 and the input of the model. Then, the output grid of all 9 configurations ([window size]s A[alphabet size]).
    Each example is completed with a comment summarizing the results of highlighted examples. The ID following the "Example :" corresponds 
    to the name of the track folder in the MoisesDB dataset.
  </p>
</div>

<!-- ðŸ”¹ Figures Section -->
<!-- <div class="figures-section">
  <h3>Main Figures</h3>
  <div class="figure">
    <img src="figures/moises_tpp_fe.png" alt="Figure 1" />
    <p class="caption">Figure 1 â€“ True Positive Percentage on MoisesDB dataset under the constrained generation setup.</p>
  </div>
  <div class="figure">
    <img src="figures/moises_lcp_fe_2.png" alt="Figure 2" />
    <p class="caption">Figure 2 â€“ Longest Common Prefix on MoisesDB dataset under the constrained generation setup.</p>
  </div>
</div> -->

<div id="audioContent"></div>

<script>
const root = 'moises';

const allConfigs = [
  "0.25s_A16", "0.35s_A16", "0.5s_A16",
  "0.25s_A64", "0.35s_A64", "0.5s_A64",
  "0.25s_A256", "0.35s_A256", "0.5s_A256"
];

const exampleSets = [
  // {
  //   name: "0611",
  //   basePath: "0611",
  //   topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
  //   examples: [
  //     { path:'moises/0611/0.35s_A16.mp3', accuracy: 27, max_len: 8, entropy:0.98 },
  //     { path:'moises/0611/0.25s_A64.mp3', accuracy:18, max_len:2, entropy:0.93 },
  //     { path:'moises/0611/0.5s_A256.mp3', accuracy:12.5, max_len:1, entropy:0.21 }
  //   ],
  //   comment: 
  //   "This is an example of lead-accompaniment interaction between two musicians, where X is the lead and Y handles the accompaniment \
  //   Despite a memory with different notes of varying duration, all responses are monotone - \
  //     no matter how large the alphabet. Interestingly, both A16 and A64 choose the same note in response,\
  //      just A16 adds a little variation. A256 repeats 2 notes over the whole response, different from \
  //      those chosen by the other alphabets, but these 2 notes are part of the same segment. \
  //      In this example, we can see that alphabet size has no significant impact on the response generated."
  // },
  {
    name: "524a",
    basePath: "524a",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      { path:'moises/524a/0.5s_A16.mp3', accuracy: 39, max_len: 10, entropy:0.7 },
      { path:'moises/524a/0.5s_A64.mp3', accuracy:10, max_len:8, entropy:2.8 },
      { path:'moises/524a/0.5s_A256.mp3', accuracy:13.5, max_len:3, entropy:2.16 }
    ],
    comment: "This is a digression example with X playinfg the main chords and Y digressing through the corresponding scale.\
    Generally speaking, all 3 alphabets tend to speak up at first, then fall silent. More specifically,\
       it looks as if these configs - in this example - mainly speak up during silences.\
      A16 presents long continuous sequences (good accuracy) and the larger the alphabet \
      the less continuous the sequences, confirming the increasing difficulty of modeling symbolic sequences with larger alphabets.\
      Counterintuitively, the accuracy of A256 is greater than A64, but still the maximum LCP is still greater for A64."
  },
  {
    name: "0e0d",
    basePath: "0e0d",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      { path:'moises/0e0d/0.25s_A16.mp3', accuracy: 15, max_len: 5, entropy:2.3 },
      { path:'moises/0e0d/0.35s_A256.mp3', accuracy:9, max_len:2, entropy:3.5 },
      { path:'moises/0e0d/0.5s_A64.mp3', accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "This is another example of lead-accompaniment interaction, where X is the leader and Y the accompaniment\
    Since the memory is composed of fine-grained notes, the smallest window is well suited for this case. The largest window (0.5 seconds) \
      is quite large and thus one segment generally contains more than one note and contrary to what is expected from the LCP figure (see paper) this \
      configuration struggles to predict\
       future tokens (max_len is small). Overall the accuracy is low, probably due to the complexity of the memory, i.e. memory uses a large \
       proportion of the vocabulary so there are few repeated tokens."
  },
  {
    name: "f4df",
    basePath: "f4df",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "This is an example of 'changement de mode' ... l'exemple ne reussit pas a changer de mode, verifier qd meme avc 0.35_16"
  },
  {
    name: "6c70",
    basePath: "6c70",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "Exemple relation harmonique avec changement de mode. Tout d'abord on peut relever que l'exemple 0.5s A16 n'est pas synchronisÃ© avec \
    la source. Ce qui indique que mÃªme une fenÃªtre de segmentation uniforme ne garanti pas la synchronisation entre la rÃ©ponse et le guide. En restant \
    sur 0.5s, on observe que plus l'a taille de l'alphabet augmente, moins al rÃ©ponse est fournie. cela est du a 2 cas de figures : choix volontaire \
    de segemnets silencieux ou pas de corresponsance dans la mÃ©moire. Ici A64 = pas de match et A256 = silence est ce que ca cofnirme que A++ -> modÃ©lisation \
    plus compliquÃ©e ? On peut relever que la config 0.35s A16 prÃ©sente des Ã©lÃ©ments harmoniques cihÃ©rents avec l'original."
  },
  {
    name: "cda4",
    basePath: "cda4",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "Exemple relation harmonique, particuliermenet avec memoire=basse. 1) generation avec basse ne marche pas bien. Ici\
    on observe que la fenetre de segmentation impacte fortement la qualitÃ© de la rÃ©ponse gÃ©nÃ©rÃ©e :; 0.5s crÃ©e des grains 'incohÃ©rents', \
    avec 90% de silence et petit dÃ©but de pluck. Donc, encodage et concatÃ©nation pas bien rÃ©ussis. Alors qu'avec 0.25s on a des segemnts \
    avec full son, gÃ©nÃ©ration pas trÃ¨s intÃ©ressante pour autant... En restant sur 0.25s et en augmentant vocab on entend des rÃ©ponses (un peu) \
    plus variÃ©es."
  },
  {
    name: "ea29",
    basePath: "ea29",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "Exemple relation harmonique, autre exemple de mem=bass. Segmentation plus cohÃ©rente avec 0.5 (appuyan tle porpose que fenetre \
    uniforme n'est pas robuste). 0.35s_A16 ne rÃ©poind que avec des segemnts videes avec click a la fin mais avec A64 on entend d'autres segemts, alors \
    que la mÃ©moire c'est la mÃªme. en restant sur 0.5s, augmenter vocab gÃ©nÃ¨re reponse + variÃ©es, et contrairement aux attentes, les rÃ©ponses avec \
    des vocab plus grands ont des meilleurs stats : A64 et 256 ont meilleure accuracy et max/median_len que A16 -> vÃ©rifier si ca vient de gt_portion."
  },
  {
    name: "0358",
    basePath: "0358",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "Exenple de suivi d'intensitÃ© / changement de mode : guitare monte en intensitÃ© quand le piano joue de maniÃ¨re plus forte sur les notes \
    (et invÃ©rsement). Pas grand chose a dire lors de la comparaison des configs, mais on observe que la config 0.5s_A16 reproduit bien l'interaction \
    originale de suivi d'intensitÃ©. rÃ©ponse 'douce/trkl' au dÃ©but puis selection de segmnts/classes avec volume/intensitÃ© + Ã©levÃ©e."
  },
  {
    name: "04f2",
    basePath: "04f2",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "relation Lead-acc, piano=lead et bass=acc. A16 ne 'marche pas' pck que les segments silencieux sont choisis par les configs. Il y a un bruit \
    de grÃ©sillement a cause de re-normalisation lors du mix. En fqit, c'est pas que ca marche pas mais petit vocab ne permet pas, dans cet exemple, de sÃ©lectioner \
    des segments non nuls. Alors que lors2que le voxqb est assez grand, le modÃ¨le peut gÃ©nÃ©rer des spÃ©cifications de classes non-silencieuses. + grand vocab -> + de choix. \
    ps : il suffit d'un segment non-silencieux pour que l'effet du bruit disparaisse (e.g. 0.35s_A256)"
  },
  {
    name: "9efa",
    basePath: "9efa",
    topFiles: ['original.mp3', 'memory.mp3', 'guide.mp3'],
    examples: [
      // { path:`moises/${name}/0.25s_A16.mp3`, accuracy: 15, max_len: 5, entropy:2.3 },
      // { path:`moises/${name}/0.35s_A256.mp3`, accuracy:9, max_len:2, entropy:3.5 },
      // { path:`moises/${name}/0.5s_A64.mp3`, accuracy:10.5, max_len:3, entropy:2.7 }
    ],
    comment: "relation Q&A. musicalement 0.5s A16 est plus complet (utilise plus de al mÃ©moire a dispo) que les autres tailles de vocab \
    en plus rÃ©ponse reproduit 'mieux' le comprotement original. A64/256 sonmt en thÃ©orie plus variÃ©s (cf entropy) mais auditivement on \
    entend moins de variations. mÃªme commentaire pour 0.35s, pas vrai pour 0.25s."
  }
];

const content = document.getElementById('audioContent');

exampleSets.forEach(set => {
  const section = document.createElement('div');
  section.className = 'category-section';

  const title = document.createElement('div');
  title.className = 'category-title';
  title.textContent = `Example: ${set.name}`;
  section.appendChild(title);

  // Top files
  const topRow = document.createElement('div');
  topRow.className = 'memory-guide-row';
  set.topFiles.forEach(file => {
    const container = document.createElement('div');
    container.className = 'audio-container';
    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = `${root}/${set.basePath}/${file}`;
    audio.preload = "none";
    const label = document.createElement('p');
    label.textContent = file.replace('.mp3', '').replace('_',' ');
    container.appendChild(label);
    container.appendChild(audio);
    topRow.appendChild(container);
  });

  if (set.comment) {
    const commentBox = document.createElement('div');
    commentBox.className = 'comment-box';
    commentBox.textContent = set.comment;
    section.appendChild(commentBox);
  }

  section.appendChild(topRow);

  // All configs
  const grid = document.createElement('div');
  grid.className = 'grid';
  allConfigs.forEach(cfg => {
    const container = document.createElement('div');
    container.className = 'audio-container';

    const highlightExample = set.examples.find(e => e.path.includes(cfg));
    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = `${root}/${set.basePath}/${cfg}.mp3`;
    audio.preload = "none";

    const label = document.createElement('p');
    label.textContent = cfg.replace("_"," ");

    const metrics = document.createElement('div');
    metrics.className = 'metrics';

    if (highlightExample) {
      metrics.innerHTML = `
        <small><b>Accuracy:</b> ${(highlightExample.accuracy).toFixed(1)}%</small><br>
        <small><b>Max Len:</b> ${highlightExample.max_len}</small><br>
        <small><b>Entropy:</b> ${highlightExample.entropy.toFixed(2)} [Bits]</small>
      `;
      container.style.fontWeight = 'bold';
      container.style.color = '#000';
    } else {
      container.style.opacity = '0.7';
    }

    container.appendChild(label);
    container.appendChild(audio);
    container.appendChild(metrics);
    grid.appendChild(container);
  });
  section.appendChild(grid);


  section.appendChild(document.createElement('hr'));
  content.appendChild(section);
});
</script>
</body>
</html>
